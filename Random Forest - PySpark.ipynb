{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline is trained on normalized counts.\n",
    "\n",
    "Three models:\n",
    "1. All features treated as continuous; 06m39s for 2014-tiny\n",
    "2. Weekday and hour treated as categorical; 07m34s for 2014-tiny\n",
    "3. Weekday, hour, and grid treated as categorical; 07m56s for 2014-tiny\n",
    "4. Grid treated as categorical (not implemented).\n",
    "\n",
    "TODO:\n",
    "- train on 2014, dev and test on 2015.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from utils import sparkutils\n",
    "\n",
    "from pyspark.sql.functions import min, max, col, stddev, abs\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, OneHotEncoderEstimator, MinMaxScaler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession.builder\\\n",
    ".appName(\"Rides Preprocessor\")\\\n",
    ".master(\"local\")\\\n",
    ".config(\"spark.local.dir\", \"/home/atkm/nycTaxi/tmp\")\\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ride_data(year, month, size='tiny'):\n",
    "    return f'data/yellow_tripdata_{year}-{month:02}_{size}.csv'\n",
    "\n",
    "def get_metar_data(year, month):\n",
    "    return f'data/metar_data/lga_{year}-{month:02}.csv'\n",
    "\n",
    "def read_csv(path):\n",
    "    return spark.read.format(\"csv\")\\\n",
    "      .option(\"header\", \"true\")\\\n",
    "      .option(\"inferSchema\", \"true\")\\\n",
    "      .load(path)\n",
    "\n",
    "def load_metar(metarPath):\n",
    "    metar = read_csv(metarPath)\n",
    "    metar = metar.select(\"valid\",\"tmpf\", \" p01i\") # note whitespace in p01i\n",
    "    return metar.withColumnRenamed('valid', 'datetime')\\\n",
    "        .withColumnRenamed('tmpf', 'fahrenheit')\\\n",
    "        .withColumnRenamed(' p01i', 'precip_in')\n",
    "\n",
    "def load_rides(ridesPath):\n",
    "    rides = read_csv(ridesPath)\n",
    "    # 2014\n",
    "    colNames = map(lambda name: name.strip(), rides.columns)\n",
    "    rides = rides.toDF(*colNames)\n",
    "    return rides.select(\"pickup_datetime\",\"pickup_latitude\", \"pickup_longitude\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = rides and metar joined.\n",
    "# model_type = '1', '2', or '3'.\n",
    "def rf_pipeline(df, model_type, grid_dict, numFolds=5):\n",
    "    \n",
    "    rf = RandomForestRegressor(\n",
    "        featuresCol='features',\n",
    "        labelCol='count_scaled'\n",
    "    )\n",
    "    \n",
    "    # Build pipeline.\n",
    "    if model_type == '1':\n",
    "        assembler = VectorAssembler(\n",
    "            inputCols = ['grid_x', 'grid_y', 'weekday', 'hour', 'fahrenheit', 'precip_in'],\n",
    "            outputCol = 'features'\n",
    "        )\n",
    "        pipeline = Pipeline(\n",
    "            stages=[assembler, rf]\n",
    "        )\n",
    "    elif model_type == '2':\n",
    "        encoder = OneHotEncoderEstimator(\n",
    "            inputCols=['weekday', 'hour'],\n",
    "            outputCols=['weekday_vec', 'hour_vec']\n",
    "        )\n",
    "        assembler = VectorAssembler(\n",
    "            inputCols = ['grid_x', 'grid_y', 'weekday_vec', 'hour_vec', 'fahrenheit', 'precip_in'],\n",
    "            outputCol = 'features'\n",
    "        )\n",
    "        pipeline = Pipeline(\n",
    "            stages=[encoder, assembler, rf]\n",
    "        )\n",
    "    elif model_type == '3':\n",
    "        encoder = OneHotEncoderEstimator(\n",
    "            inputCols=['weekday','hour','grid_x','grid_y'],\n",
    "            outputCols=['weekday_vec','hour_vec','grid_x_vec','grid_y_vec']\n",
    "        )\n",
    "        assembler = VectorAssembler(\n",
    "            inputCols = ['grid_x_vec', 'grid_y_vec', 'weekday_vec', 'hour_vec', 'fahrenheit', 'precip_in'],\n",
    "            outputCol = 'features'\n",
    "        )\n",
    "        pipeline = Pipeline(\n",
    "            stages=[encoder, assembler, rf]\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Model type must be either 1, 2, or 3.\")\n",
    "        \n",
    "    # TODO: is there a random search module?\n",
    "    # start with numTrees: [10, 100, 1000]\n",
    "    # maxDepth: [10, 100, 1000]\n",
    "    # minInstancesPerNode: [1, 10, 100, 1000]\n",
    "    paramGrid = ParamGridBuilder() \\\n",
    "        .addGrid(rf.numTrees, grid_dict['numTrees']) \\\n",
    "        .addGrid(rf.maxDepth, grid_dict['maxDepth']) \\\n",
    "        .addGrid(rf.minInstancesPerNode, grid_dict['minInstancesPerNode']) \\\n",
    "        .build()\n",
    "    \n",
    "    \n",
    "    #train, dev, test = joined.randomSplit([1/3] * 3)\n",
    "    train, dev, test = joined.randomSplit([0.8, 0.1, 0.1])\n",
    "    \n",
    "    evaluator = RegressionEvaluator(\n",
    "        labelCol='count_scaled', predictionCol='prediction', metricName='rmse'\n",
    "    )\n",
    "\n",
    "    crossval = CrossValidator(estimator=pipeline,\n",
    "                         estimatorParamMaps=paramGrid,\n",
    "                         evaluator=evaluator,\n",
    "                         numFolds=numFolds)\n",
    "    \n",
    "    model = crossval.fit(train)\n",
    "    pred_on_test = model.transform(test)\n",
    "    rmse_on_test = evaluator.evaluate(pred_on_test)\n",
    "    # TODO: scale back to make these values interpretable\n",
    "    mean_error = pred.agg(pyspark.sql.functions.abs(col('count') - col('prediction')))\n",
    "    error_stddev = pred.agg(stddev(col('count') - col('prediction')))\n",
    "    \n",
    "    # get metric values with model.avgMetrics,\n",
    "    # and parameters with model.getEstimatorParamMaps\n",
    "    return model, pred_on_test, rmse_on_test#, mean_error, error_stddev\n",
    "\n",
    "def get_model_stats(model):\n",
    "    \n",
    "    def _parse_params(params):\n",
    "        for p, val in params.items():\n",
    "            yield (p.name, val)\n",
    "            \n",
    "    pmaps = model.getEstimatorParamMaps()\n",
    "    pmaps = [list(_parse_params(params)) for params in pmaps]\n",
    "    return list(zip(pmaps, model.avgMetrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rides = sparkutils.count_rides(\n",
    "    load_rides(get_ride_data(2014,1))\n",
    ")\n",
    "metar = sparkutils.clean_metar(\n",
    "    load_metar(get_metar_data(2014,1))\n",
    ")\n",
    "for m in range(1): # TODO: replace range(1) with range(12)\n",
    "    rides = rides.unionAll(\n",
    "        sparkutils.count_rides(load_rides(get_ride_data(2014,m+2)))\n",
    "    )\n",
    "    metar = metar.unionAll(\n",
    "        sparkutils.clean_metar(load_metar(get_metar_data(2014,m+2)))\n",
    "    )\n",
    "    \n",
    "joined = sparkutils.join_rides_metar(rides,metar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+------+-----+-------------------+\n",
      "|    pickup_datetime|grid_x|grid_y|count|       count_scaled|\n",
      "+-------------------+------+------+-----+-------------------+\n",
      "|2014-01-24 09:00:00|     5|    28|    1|0.16666666666666666|\n",
      "|2014-01-29 22:00:00|     6|    25|    2| 0.3333333333333333|\n",
      "+-------------------+------+------+-----+-------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-------------------+----------+---------+\n",
      "|           datetime|fahrenheit|precip_in|\n",
      "+-------------------+----------+---------+\n",
      "|2014-01-27 03:00:00|     30.02|      0.0|\n",
      "|2014-01-10 21:00:00|     33.98|      0.0|\n",
      "+-------------------+----------+---------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+------+------+-----+-------------------+----------+---------+-------+----+\n",
      "|grid_x|grid_y|count|       count_scaled|fahrenheit|precip_in|weekday|hour|\n",
      "+------+------+-----+-------------------+----------+---------+-------+----+\n",
      "|     5|    30|    1|0.16666666666666666|     30.02|      0.0|      1|   3|\n",
      "|     4|    26|    1|0.16666666666666666|     30.02|      0.0|      1|   3|\n",
      "|     5|    25|    2| 0.3333333333333333|     30.02|      0.0|      1|   3|\n",
      "|     6|    26|    1|0.16666666666666666|     33.98|      0.0|      5|  21|\n",
      "|     6|    28|    1|0.16666666666666666|     33.98|      0.0|      5|  21|\n",
      "+------+------+-----+-------------------+----------+---------+-------+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "15360\n"
     ]
    }
   ],
   "source": [
    "rides.show(2)\n",
    "metar.show(2)\n",
    "joined.show(5)\n",
    "print(joined.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trained Random Forest Model - PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: all features treated as continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.26 s, sys: 1.45 s, total: 5.71 s\n",
      "Wall time: 15min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "grid_dict = {'numTrees': [5],\n",
    "             'maxDepth': [10],\n",
    "             'minInstancesPerNode': [1,5]}\n",
    "# cv=5, 2 parameter sets to search, 2 months => 29m32s\n",
    "# cv=2, 2 parameter sets to search, 2 months => 11m40s\n",
    "model, pred, rmse, mean_error, error_stddev = rf_pipeline(joined, '1', grid_dict, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([([('numTrees', 5), ('maxDepth', 10), ('minInstancesPerNode', 1)],\n",
       "   0.09575448006144924),\n",
       "  ([('numTrees', 5), ('maxDepth', 10), ('minInstancesPerNode', 5)],\n",
       "   0.09537282127403517)],\n",
       " 0.0957761876207309)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_model_stats(model), rmse, mean_error, error_stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+\n",
      "|stddev_samp((count - prediction))|\n",
      "+---------------------------------+\n",
      "|               0.5935841436292253|\n",
      "+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred.agg(stddev(col('count') - col('prediction'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CrossValidatorModel' object has no attribute 'stages'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-c52da95e8325>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{i}: {s}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CrossValidatorModel' object has no attribute 'stages'"
     ]
    }
   ],
   "source": [
    "for i, s in enumerate(model.stages):\n",
    "    print(f'{i}: {s}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: weekday and hour as categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.1 s, sys: 3.76 s, total: 14.8 s\n",
      "Wall time: 26min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "grid_dict = {'numTrees': [5],\n",
    "             'maxDepth': [10],\n",
    "             'minInstancesPerNode': [1,5]}\n",
    "# cv=2, 2 parameter sets to search, 2 months => 26m40s\n",
    "model, pred, rmse = rf_pipeline(joined, '2', grid_dict, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([('numTrees', 5), ('maxDepth', 10), ('minInstancesPerNode', 1)],\n",
       "  0.5630926358762378),\n",
       " ([('numTrees', 5), ('maxDepth', 10), ('minInstancesPerNode', 5)],\n",
       "  0.5645315490127796)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_model_statsdel_stats(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoderEstimator(\n",
    "    inputCols=['weekday', 'hour'],\n",
    "    outputCols=['weekday_vec', 'hour_vec']\n",
    ")\n",
    "feature_assembler = VectorAssembler(\n",
    "    inputCols = ['grid_x', 'grid_y', 'weekday_vec', 'hour_vec', 'fahrenheit', 'precip_in'],\n",
    "    outputCol = 'features'\n",
    ")\n",
    "# target_scaler = MinMaxScaler(\n",
    "#     inputCol=\"count\",\n",
    "#     outputCol=\"count_scaled\"\n",
    "# )\n",
    "rf = RandomForestRegressor(\n",
    "    featuresCol='features',\n",
    "    labelCol='count_scaled',\n",
    "    numTrees=3, #default = 20\n",
    "    maxDepth=5, #default = 5\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    stages=[encoder, feature_assembler, rf]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['grid_x', 'grid_y', 'count', 'count_scaled', 'fahrenheit', 'precip_in', 'weekday', 'hour', 'weekday_vec', 'hour_vec', 'features', 'prediction']\n",
      "0.11078096256449929\n",
      "CPU times: user 371 ms, sys: 93.2 ms, total: 464 ms\n",
      "Wall time: 26.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train, test = joined.randomSplit([0.5,0.5])\n",
    "model = pipeline.fit(train)\n",
    "pred = model.transform(test)\n",
    "print(pred.columns)\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol='count_scaled', predictionCol='prediction', metricName='rmse'\n",
    ")\n",
    "rmse = evaluator.evaluate(pred)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: weekday, hour, and grid as categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoderEstimator(\n",
    "    inputCols=['weekday','hour','grid_x','grid_y'],\n",
    "    outputCols=['weekday_vec','hour_vec','grid_x_vec','grid_y_vec']\n",
    ")\n",
    "assembler = VectorAssembler(\n",
    "    inputCols = ['grid_x_vec', 'grid_y_vec', 'weekday_vec', 'hour_vec', 'fahrenheit', 'precip_in'],\n",
    "    outputCol = 'features'\n",
    ")\n",
    "rf = RandomForestRegressor(\n",
    "    featuresCol='features',\n",
    "    labelCol='count',\n",
    "    numTrees=3, #default = 20\n",
    "    maxDepth=5, #default = 5\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    stages=[encoder, assembler, rf]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['grid_x', 'grid_y', 'count', 'fahrenheit', 'precip_in', 'weekday', 'hour', 'weekday_vec', 'hour_vec', 'grid_x_vec', 'grid_y_vec', 'features', 'prediction']\n",
      "0.5673652391255603\n",
      "CPU times: user 3.51 s, sys: 1.35 s, total: 4.86 s\n",
      "Wall time: 7min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train, test = joined.randomSplit([0.5,0.5])\n",
    "model = pipeline.fit(train)\n",
    "pred = model.transform(test)\n",
    "print(pred.columns)\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol='count', predictionCol='prediction', metricName='rmse'\n",
    ")\n",
    "rmse = evaluator.evaluate(pred)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
